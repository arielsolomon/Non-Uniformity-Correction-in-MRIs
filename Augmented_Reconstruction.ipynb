{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Augmented Reconstruction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqG13qPsW7g-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Averaging the results obtained from the models trained on 3 orthogonal axes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmWexsvAXZ7t",
        "colab_type": "code",
        "outputId": "ea8e2da8-17ed-47b4-f546-502755007baa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khkz-oH6XaGS",
        "colab_type": "code",
        "outputId": "882a8009-0810-4f17-94d1-6b9411a1be33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import nibabel as nib #reading MR images\n",
        "import glob\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "!pip install nilearn\n",
        "import nilearn as nil\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nilearn in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from nilearn) (1.17.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from nilearn) (0.14.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.6/dist-packages (from nilearn) (0.21.3)\n",
            "Requirement already satisfied: nibabel>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from nilearn) (2.3.3)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from nilearn) (0.0)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.6/dist-packages (from nilearn) (1.3.3)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from nibabel>=2.0.2->nilearn) (1.12.0)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from nibabel>=2.0.2->nilearn) (0.98)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAcKmQnuaJVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ff = sorted(glob.glob('drive/My Drive/Research/Skull_Stripped_training_input_images/*'))   #For reading a folder\n",
        "ff = ff[1:]\n",
        "\n",
        "N=64    #resized image dimensions (N, N)\n",
        "\n",
        "import cv2\n",
        "images_train = []    #contains the training data sliced over all 3 axes\n",
        "\n",
        "#Have used nibabel library which has been imported as 'nib' to read the Nifti images \n",
        "\n",
        "for f in range(len(ff)):\n",
        "    a = nib.load(ff[f])\n",
        "    a = a.get_fdata()\n",
        "    for k in range(a.shape[0]):                                                                                                   #for k in range(a.shape[0]):\n",
        "        temp = cv2.resize(a[k,:,:], (N, N), interpolation = cv2.INTER_LINEAR)        #INTER_CUBIC  (type of interpolation)                                     #temp = cv2.resize(a[:,100+k,:], (N, N), interpolation = cv2.INTER_CUBIC) \n",
        "        images_train.append(temp)\n",
        "\n",
        "images_train = np.asarray(images_train)  #converting list to a numpy array (first dimension refers to the #of samples and the second and the third are the 2D image dimension)\n",
        "images_train = images_train.reshape(-1,N,N,1)  #-1 is generally used for an unknown dimension\n",
        "\n",
        "m_x = np.max(images_train)    #Normalization over all the training images to get the intensity values between 0 and 1 \n",
        "mi_x = np.min(images_train)\n",
        "\n",
        "images_train_normalized_x = (images_train - mi_x) / (m_x - mi_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3aFQEndaJdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ff = sorted(glob.glob('drive/My Drive/Research/Skull_Stripped_training_input_images/*'))   #For reading a folder\n",
        "# N=64    #resized image dimensions (N, N)\n",
        "# ff = ff[1:]\n",
        "\n",
        "# import cv2\n",
        "# images_train = []    #contains the training data sliced over all 3 axes\n",
        "\n",
        "# #Have used nibabel library which has been imported as 'nib' to read the Nifti images \n",
        "\n",
        "# for f in range(len(ff)):\n",
        "#     a = nib.load(ff[f])\n",
        "#     a = a.get_fdata()   \n",
        "#     for k in range(a.shape[1]):\n",
        "#         temp = cv2.resize(a[:,k,:], (N, N), interpolation = cv2.INTER_LINEAR) \n",
        "#         images_train.append(temp)\n",
        "        \n",
        "# images_train = np.asarray(images_train)  #converting list to a numpy array (first dimension refers to the #of samples and the second and the third are the 2D image dimension)\n",
        "# images_train = images_train.reshape(-1,N,N,1)  #-1 is generally used for an unknown dimension\n",
        "\n",
        "# m_y = np.max(images_train)    #Normalization over the ground truth for all the training images to get the intensity values between 0 and 1\n",
        "# mi_y = np.min(images_train)\n",
        "\n",
        "# images_train_normalized_y = (images_train - mi_y) / (m_y - mi_y)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46cb5ZjraJnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ff = sorted(glob.glob('drive/My Drive/Research/Skull_Stripped_training_input_images/*'))   #For reading a folder\n",
        "# N=64    #resized image dimensions (N, N)\n",
        "# ff = ff[1:]\n",
        "\n",
        "# import cv2\n",
        "# images_train = []    #contains the training data sliced over all 3 axes\n",
        "\n",
        "# #Have used nibabel library which has been imported as 'nib' to read the Nifti images \n",
        "\n",
        "# for f in range(len(ff)):\n",
        "#     a = nib.load(ff[f])\n",
        "#     a = a.get_fdata()   \n",
        "#     for k in range(a.shape[2]):\n",
        "#         temp = cv2.resize(a[:,:,k], (N, N), interpolation = cv2.INTER_LINEAR) \n",
        "#         images_train.append(temp)\n",
        "# images_train = np.asarray(images_train)  #converting list to a numpy array (first dimension refers to the #of samples and the second and the third are the 2D image dimension)\n",
        "# images_train = images_train.reshape(-1,N,N,1)  #-1 is generally used for an unknown dimension\n",
        "\n",
        "# m_z = np.max(images_train)    #Normalization over all the training images to get the intensity values between 0 and 1 \n",
        "# mi_z = np.min(images_train)\n",
        "\n",
        "# images_train_normalized_z = (images_train - mi_z) / (m_z - mi_z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZPqXBeZKdtk",
        "colab_type": "code",
        "outputId": "e44ea6b8-0721-481a-8bdb-d241c613efbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "ff"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_0.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_1.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_10.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_11.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_12.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_13.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_14.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_15.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_16.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_17.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_18.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_19.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_2.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_20.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_21.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_22.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_23.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_24.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_25.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_26.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_27.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_28.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_29.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_3.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_30.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_31.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_32.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_33.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_34.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_35.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_36.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_37.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_38.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_39.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_4.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_5.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_6.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_7.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_8.nii.gz',\n",
              " 'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_9.nii.gz']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKXC5Oy3XaOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model #load the model that has been trained previously on 3 orthogonal axes separately\n",
        "\n",
        "model_x = load_model('drive/My Drive/Research/Models/autoencoder_Augmented_model.h5')\n",
        "# model_y = load_model('drive/My Drive/Research/autoencoder_model_yaxis.h5')\n",
        "# model_z = load_model('drive/My Drive/Research/autoencoder_model_zaxis.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTsC3Uo1XaT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_x = model_x.predict(images_train_normalized_x)\n",
        "# pred_y = model_y.predict(images_train_normalized_y)\n",
        "# pred_z = model_z.predict(images_train_normalized_z)\n",
        "\n",
        "m_x = 2.776857385470066 \n",
        "mi_x = 0.3794334704289213\n",
        "\n",
        "m_y = 2.776857385470066 \n",
        "mi_y = 0.3794334704289213\n",
        "\n",
        "m_z = 2.776857385470066 \n",
        "mi_z = 0.3794334704289213\n",
        "\n",
        "# Un-normalizing the data\n",
        "pred_x_unnormalized=(pred_x* (m_x - mi_x) + mi_x)\n",
        "# pred_y_unnormalized=(pred_y* (m_y - mi_y) + mi_y)\n",
        "# pred_z_unnormalized=(pred_z* (m_z - mi_z) + mi_z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4OCnv--CugX",
        "colab_type": "code",
        "outputId": "a335a650-0074-46ed-bf9b-05b026ce52b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(pred_x_unnormalized.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10140, 64, 64, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnkvSYqAkimq",
        "colab_type": "code",
        "outputId": "9125d036-88cb-4268-a89e-1b77b1de803f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ff[9]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_18.nii.gz'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmfDyKcCZxY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reconstructing bias field on 3 orthogonal axes for one brain volume (can be extended over every volume using a for loop)\n",
        "\n",
        "f=9  #For a single image\n",
        "temp=np.empty([260,311,260])\n",
        "reconstructed_bias_x=[] \n",
        "for idx in range(260*f,260*(f+1)):\n",
        "      img = pred_x_unnormalized[idx, :, :,0]\n",
        "      img_resized = cv2.resize(img, (260, 311), interpolation=cv2.INTER_CUBIC)\n",
        "      temp[idx%260,:,:]=img_resized\n",
        "reconstructed_bias_x.append(temp)\n",
        "\n",
        "# f=9   #For a single image\n",
        "# temp=np.empty([260,311,260])\n",
        "# reconstructed_bias_y=[] \n",
        "# for idx in range(311*f,311*(f+1)):\n",
        "#       img = pred_y_unnormalized[idx, :, :,0]\n",
        "#       img_resized = cv2.resize(img, (260, 260), interpolation=cv2.INTER_CUBIC)\n",
        "#       temp[:,idx%311,:]=img_resized\n",
        "# reconstructed_bias_y.append(temp)\n",
        "\n",
        "# f=9   #For a single image\n",
        "# temp=np.empty([260,311,260])\n",
        "# reconstructed_bias_z=[] \n",
        "# for idx in range(260*f,260*(f+1)):\n",
        "#       img = pred_z_unnormalized[idx, :, :,0]\n",
        "#       img_resized = cv2.resize(img, (311,260), interpolation=cv2.INTER_CUBIC)\n",
        "#       temp[:,:,idx%260]=img_resized\n",
        "# reconstructed_bias_z.append(temp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEsugvmpC7YO",
        "colab_type": "code",
        "outputId": "b4b6b67a-38d5-4fa6-d5f6-dbc0bf61fbf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "reconstructed_bias_z[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(260, 311, 260)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwEucwftZxUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_reconstructed_bias = reconstructed_bias_x #[]\n",
        "#for f in range(len(ff)):\n",
        "#For a single case\n",
        "# f=0\n",
        "# average_reconstructed_bias.append((reconstructed_bias_x[f] + reconstructed_bias_y[f] + reconstructed_bias_z[f])/3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td_qbNz2y5ig",
        "colab_type": "code",
        "outputId": "afb30c58-75d1-4f71-f27e-c32d44896e45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(average_reconstructed_bias)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJgbiRzswil-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ff = sorted(glob.glob('drive/My Drive/Research/Skull_Stripped_training_input_images/*'))   #For reading a folder\n",
        "ff= ff[1:]\n",
        "input_images = []\n",
        "f= 9\n",
        "a = nib.load(ff[f])\n",
        "a = a.get_fdata()  \n",
        "input_images.append(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGXPyBzrjVms",
        "colab_type": "code",
        "outputId": "94fd03dc-5a1e-45df-e981-9ead41640f31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ff[f]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'drive/My Drive/Research/Skull_Stripped_training_input_images/training_input_18.nii.gz'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0qneRk-ZxO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For a single image\n",
        "f=0\n",
        "corrected_images=[]\n",
        "temp=np.empty([260,311,260])\n",
        "for i in range(260):\n",
        "  for j in range(311):\n",
        "    for k in range(260):\n",
        "      if average_reconstructed_bias[f][i,j,k]==0:\n",
        "          temp[i,j,k] = input_images[0][i,j,k]\n",
        "      else:\n",
        "          temp[i,j,k] = input_images[0][i,j,k]/average_reconstructed_bias[f][i,j,k]\n",
        "corrected_images.append(temp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSjJRVcYZxGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Corrected MRI images\n",
        "corrected_img = nil.image.new_img_like(nib.load(ff[f]),corrected_images[0],copy_header=True)\n",
        "nib.save(corrected_img,'drive/My Drive/Research/average_corrected_mri.nii.gz')\n",
        "\n",
        "#Estimated bias field\n",
        "reconstructed_img = nil.image.new_img_like(nib.load(ff[f]),average_reconstructed_bias[0],copy_header=True)\n",
        "nib.save(reconstructed_img,'drive/My Drive/Research/average_reconstructed_bias.nii.gz')\n",
        "\n",
        "#Estimated bias field\n",
        "input_img = nil.image.new_img_like(nib.load(ff[f]),input_images[0],copy_header=True)\n",
        "nib.save(input_img,'drive/My Drive/Research/input_image.nii.gz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp28QR8lzwoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(np.max(corrected_images),np.min(corrected_images))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhYLbAHNBoqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(np.max(average_reconstructed_bias),np.min(average_reconstructed_bias))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8ebQHkYE67E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ai3-SWNB7wB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}